using ScraperBackendService.Configuration;using ScraperBackendService.Extensions;using ScraperBackendService.Middleware;using ScraperBackendService.Models;using ScraperBackendService.Providers.DLsite;using ScraperBackendService.Providers.Hanime;using ScraperBackendService.Core.Logging;using ScraperBackendService.Core.Caching;using ScraperBackendService.Core.Concurrency;using System.Text.Json;/// <summary>/// Main entry point for the Scraper Backend Service./// Configures and starts a web API server for content scraping operations with rate limiting, /// concurrency control, caching, and comprehensive error handling./// </summary>/// <remarks>/// This service provides REST API endpoints for scraping content metadata from various providers./// Key features:/// - Provider-specific concurrency control and rate limiting/// - Intelligent caching with TTL support/// - Authentication middleware support/// - Memory optimization and garbage collection/// - Comprehensive logging with structured event categories/// - Graceful error handling and recovery/// </remarks>/// <example>/// Usage examples:////// // Run with default settings/// dotnet run////// // Run on custom port/// dotnet run 9090////// // Run with environment variables/// set SCRAPER_PORT=8080/// set SCRAPER_AUTH_TOKEN=my-secret-token/// dotnet run////// // API endpoints available:/// GET /                           - Service info and health status/// GET /health                     - Health check endpoint/// GET /api/hanime/search?title=   - Search Hanime content by title (strict search mode)/// GET /api/hanime/{id}            - Get Hanime content details by specific ID/// GET /api/dlsite/search?title=   - Search DLsite content by title (strict search mode)/// GET /api/dlsite/{id}            - Get DLsite content details by specific ID/// /// Note: Search endpoints now strictly treat input as search keywords./// Even if the input looks like an ID (e.g., "123456" or "RJ123456"), /// it will be used as a search term rather than switching to ID-based retrieval./// </example>var builder = WebApplication.CreateBuilder(args);// Load configuration from appsettings.json and bind to ServiceConfigurationvar serviceConfig = new ServiceConfiguration();builder.Configuration.GetSection(ServiceConfiguration.SectionName).Bind(serviceConfig);// Override with command line arguments if providedif (args.Length > 0 && int.TryParse(args[0], out var port))    serviceConfig.Port = port;// Override with environment variables if setvar envPort = Environment.GetEnvironmentVariable("SCRAPER_PORT");if (!string.IsNullOrEmpty(envPort) && int.TryParse(envPort, out var ePort))    serviceConfig.Port = ePort;var envToken = Environment.GetEnvironmentVariable("SCRAPER_AUTH_TOKEN");if (!string.IsNullOrEmpty(envToken))    serviceConfig.AuthToken = envToken;// Configure logging based on service configurationbuilder.Logging.ClearProviders();builder.Logging.AddConsole();// Configure precise logging levels to reduce noisebuilder.Logging.AddFilter("Microsoft.AspNetCore", LogLevel.Warning);builder.Logging.AddFilter("Microsoft.Extensions.Hosting", LogLevel.Warning);builder.Logging.AddFilter("Microsoft.AspNetCore.Hosting.Diagnostics", LogLevel.Error);builder.Logging.AddFilter("Microsoft.AspNetCore.Routing.EndpointMiddleware", LogLevel.Error);builder.Logging.AddFilter("Microsoft.AspNetCore.StaticFiles", LogLevel.Error);builder.Logging.AddFilter("Microsoft", LogLevel.Warning);builder.Logging.AddFilter("System", LogLevel.Warning);// Set application logging level to Information by defaultbuilder.Logging.SetMinimumLevel(LogLevel.Information);builder.Logging.AddFilter("ScraperBackendService", LogLevel.Information);// Register scraping services and dependenciesbuilder.Services.AddScrapingServices(serviceConfig);// Register metadata cache as singletonbuilder.Services.AddSingleton<MetadataCache>();// Register Playwright cleanup hosted servicebuilder.Services.AddHostedService<PlaywrightCleanupService>();// Configure JSON serialization for API responsesbuilder.Services.Configure<JsonSerializerOptions>(options =>{    options.PropertyNamingPolicy = JsonNamingPolicy.CamelCase;    options.WriteIndented = true;});var app = builder.Build();// Add token authentication middlewareapp.UseMiddleware<TokenAuthenticationMiddleware>();// Add memory optimization middleware to prevent memory bloatapp.UseMiddleware<MemoryOptimizationMiddleware>();// Configure request timeout middlewareapp.Use(async (context, next) =>{    using var cts = new CancellationTokenSource(TimeSpan.FromSeconds(serviceConfig.RequestTimeoutSeconds));    var combinedToken = CancellationTokenSource.CreateLinkedTokenSource(        context.RequestAborted, cts.Token).Token;    context.RequestAborted = combinedToken;    await next();});var logger = app.Services.GetRequiredService<ILogger<Program>>();// Global exception handler middleware - logs unhandled exceptions from request pipelineapp.UseExceptionHandler(errorApp =>{    errorApp.Run(async context =>    {        var feature = context.Features.Get<Microsoft.AspNetCore.Diagnostics.IExceptionHandlerFeature>();        var ex = feature?.Error;        if (ex != null)        {            logger.LogError(ex, "Unhandled exception in request pipeline");        }        context.Response.StatusCode = 500;        await context.Response.WriteAsync("Internal server error");    });});// Subscribe to process-level and task-level unobserved exceptions to ensure they are loggedAppDomain.CurrentDomain.UnhandledException += (s, e) =>{    if (e.ExceptionObject is Exception ex)    {        logger.LogError(ex, "Unhandled domain exception");    }    else    {        logger.LogError("Unhandled domain exception: {Obj}", e.ExceptionObject?.ToString() ?? "<null>");    }};TaskScheduler.UnobservedTaskException += (s, e) =>{    logger.LogError(e.Exception, "Unobserved task exception");    e.SetObserved();};// Service information and health check endpointsapp.MapGet("/", () =>{    return Results.Json(ApiResponse<ServiceInfo>.Ok(new ServiceInfo    {        AuthEnabled = !string.IsNullOrWhiteSpace(serviceConfig.AuthToken)    }));});app.MapGet("/health", () => Results.Json(new { status = "healthy", timestamp = DateTime.UtcNow }));// Cache statistics endpoint for monitoring cache performanceapp.MapGet("/cache/stats", (MetadataCache cache) =>{    var stats = cache.GetStatistics();    return Results.Json(new    {        hitCount = stats.HitCount,        missCount = stats.MissCount,        evictionCount = stats.EvictionCount,        totalRequests = stats.TotalRequests,        hitRatio = $"{stats.HitRatio:P2}",        timestamp = DateTime.UtcNow    });});// Cache management endpointsapp.MapDelete("/cache/clear", (MetadataCache cache) =>{    cache.Clear();    return Results.Json(new { message = "Cache cleared successfully", timestamp = DateTime.UtcNow });});app.MapDelete("/cache/{provider}/{id}", (string provider, string id, MetadataCache cache) =>{    cache.Remove(provider, id);    return Results.Json(new { message = $"Cache entry removed for {provider}:{id}", timestamp = DateTime.UtcNow });});// Add redirect endpoint for DLsite external linksapp.MapGet("/r/dlsite/{id}", (string id) =>{    try    {        var target = ScraperBackendService.Core.Routing.IdParsers.BuildDlsiteDetailUrl(id);        return Results.Redirect(target);    }    catch (Exception ex)    {        logger.LogWarning(ex, "Failed to build DLsite redirect for id={Id}", id);        return Results.NotFound();    }});// =============== Hanime API Endpoints ===============/// <summary>/// Search for Hanime content by title with optional filtering and result limiting./// This endpoint performs content search using the provided title as a search term,/// then fetches detailed metadata for each result concurrently./// </summary>/// <param name="title">Search title or keyword. Will be treated as search text regardless of format.</param>/// <param name="max">Maximum number of results to return (default: 12, max: 50)</param>/// <param name="genre">Genre filter (optional, not currently implemented)</param>/// <param name="sort">Sort order (optional, not currently implemented)</param>/// <param name="provider">Hanime provider instance (dependency injected)</param>/// <param name="hanimeLimiter">Hanime concurrency limiter for resource management</param>/// <param name="hanimeRateLimiter">Hanime rate limiter for request throttling</param>/// <param name="ct">Cancellation token for request timeout handling</param>/// <returns>JSON response containing list of Hanime metadata objects</returns>/// <example>/// GET /api/hanime/search?title=Love&max=5/// GET /api/hanime/search?title=123456&max=5 (will search for "123456" as text, not as ID)/// </example>app.MapGet("/api/hanime/search", async (    string title,    int? max,    string? genre,    string? sort,    HanimeProvider provider,    ScraperBackendService.Core.Concurrency.HanimeConcurrencyLimiter hanimeLimiter,    ScraperBackendService.Core.Concurrency.HanimeRateLimiter hanimeRateLimiter,    CancellationToken ct) =>{    ConcurrencySlot? slot = null;        try    {        // Log user operation start        logger.LogAlways("HanimeSearch", $"Searching: '{title}'");                // Wait for available slot (up to 15 seconds)        slot = await hanimeLimiter.TryWaitAndAcquireSlotAsync(15000, ct).ConfigureAwait(false);        if (slot == null)        {            // Rate limit exceeded - return 429 status            logger.LogRateLimit("HanimeSearch");            return Results.Json(                ApiResponse<List<Metadata>>.Fail("Service busy. All concurrency slots occupied. Please retry later."),                statusCode: 429);        }                logger.LogDebug("[HanimeSearch] Acquired slot {SlotId} for search: '{Title}'", slot.SlotId, title);                // Apply rate limiting for this slot before searching        var waitTime = hanimeRateLimiter.GetWaitTime(slot.SlotId);        if (waitTime > TimeSpan.Zero)        {            // Log rate limit wait time            logger.LogAlways("HanimeSearch", $"Waiting {waitTime.TotalSeconds:F0}s (rate limit)");        }                await hanimeRateLimiter.WaitIfNeededAsync(slot.SlotId, ct).ConfigureAwait(false);                // Now perform the search        var maxResults = Math.Min(max ?? 12, 50);        var hits = await provider.SearchAsync(title, maxResults, ct);        var results = new List<Metadata>();                // Fetch details for search results (internal concurrent fetching without additional rate limiting)        var tasks = hits.Take(maxResults).Select(async hit =>        {            try            {                var detail = await provider.FetchDetailAsync(hit.DetailUrl, ct).ConfigureAwait(false);                if (detail != null)                {                    lock (results)                    {                        results.Add(detail);                    }                }            }            catch (Exception)            {                logger.LogWarningLite("DetailFetch", "Failed", hit.DetailUrl);            }        });                await Task.WhenAll(tasks).ConfigureAwait(false);                // Log operation result        if (results.Count > 0)        {            logger.LogAlways("HanimeSearch", $"Found {results.Count} results");        }        else        {            logger.LogAlways("HanimeSearch", "No results found");        }                return Results.Json(ApiResponse<List<Metadata>>.Ok(results));    }    catch (OperationCanceledException)    {        logger.LogWarningLite("HanimeSearch", "Cancelled", title);        return Results.Json(ApiResponse<List<Metadata>>.Fail("Request cancelled"));    }    catch (Exception ex)    {        logger.LogFailure("HanimeSearch", $"Search error: {ex.Message}", title, ex);        return Results.Json(ApiResponse<List<Metadata>>.Fail($"Search error: {ex.Message}"));    }    finally    {        slot?.Dispose();    }});/// <summary>/// Get detailed information for a specific Hanime content by ID./// This endpoint first checks the cache, then fetches from the provider if needed./// Implements two-stage cache checking to minimize concurrency slot usage./// </summary>/// <param name="id">Hanime content ID to retrieve</param>/// <param name="provider">Hanime provider instance (dependency injected)</param>/// <param name="hanimeLimiter">Hanime concurrency limiter for resource management</param>/// <param name="hanimeRateLimiter">Hanime rate limiter for request throttling</param>/// <param name="cache">Metadata cache instance for result caching</param>/// <param name="ct">Cancellation token for request timeout handling</param>/// <returns>JSON response containing Hanime metadata object or error</returns>app.MapGet("/api/hanime/{id}", async (    string id,    HanimeProvider provider,    ScraperBackendService.Core.Concurrency.HanimeConcurrencyLimiter hanimeLimiter,    ScraperBackendService.Core.Concurrency.HanimeRateLimiter hanimeRateLimiter,    MetadataCache cache,    CancellationToken ct) =>{    ConcurrencySlot? slot = null;        try    {        // Log user operation start        logger.LogAlways("HanimeDetail", $"Query: '{id}'");                if (!provider.TryParseId(id, out var parsedId))        {            logger.LogWarningLite("HanimeDetail", "Invalid ID format", id);            return Results.Json(ApiResponse<Metadata>.Fail($"Invalid Hanime ID: {id}"));        }                // First, check cache without acquiring concurrency slot        var cachedResult = cache.TryGetCached("hanime", parsedId);        if (cachedResult != null)        {            // Cache hit - no need for concurrency slot            logger.LogInformation("[HanimeDetail] Cache hit: '{Id}'", id);                        // Log operation result            logger.LogAlways("HanimeDetail", $"✅ Found (cache)");            return Results.Json(ApiResponse<Metadata>.Ok(cachedResult));        }                // Cache miss - now we need to acquire concurrency slot        slot = await hanimeLimiter.TryWaitAndAcquireSlotAsync(15000, ct).ConfigureAwait(false);        if (slot == null)        {            // Rate limit exceeded - return 429 status            logger.LogRateLimit("HanimeDetail");            return Results.Json(                ApiResponse<Metadata>.Fail("Service busy. All concurrency slots occupied. Please retry later."),                 statusCode: 429);        }                logger.LogDebug("[HanimeDetail] Acquired slot {SlotId} for '{Id}'", slot.SlotId, id);                // Double-check cache after acquiring slot (in case another request cached it)        cachedResult = cache.TryGetCached("hanime", parsedId);        if (cachedResult != null)        {            logger.LogInformation("[HanimeDetail] Cache hit after slot: '{Id}'", id);            logger.LogAlways("HanimeDetail", $"✅ Found (cache)");            return Results.Json(ApiResponse<Metadata>.Ok(cachedResult));        }                // Cache still empty, apply rate limiting for this slot before fetching        var waitTime = hanimeRateLimiter.GetWaitTime(slot.SlotId);        if (waitTime > TimeSpan.Zero)        {            // Log rate limit wait time            logger.LogAlways("HanimeDetail", $"Waiting {waitTime.TotalSeconds:F0}s (rate limit)");        }                await hanimeRateLimiter.WaitIfNeededAsync(slot.SlotId, ct).ConfigureAwait(false);                // Now fetch from provider        var detailUrl = provider.BuildDetailUrlById(parsedId);        var result = await provider.FetchDetailAsync(detailUrl, ct);                // Cache the result (even if null)        cache.SetCached("hanime", parsedId, result);                // Force garbage collection after ID query to release resources        var memoryBefore = GC.GetTotalMemory(false);        GC.Collect();        GC.WaitForPendingFinalizers();        GC.Collect();        var memoryAfter = GC.GetTotalMemory(true);        var freedMemory = memoryBefore - memoryAfter;                // Log memory cleanup at debug level        logger.LogResourceEvent("MemoryCleanup", $"Freed {freedMemory / 1024 / 1024}MB", "HanimeDetail");                // Log operation result        if (result != null)        {            logger.LogAlways("HanimeDetail", "✅ Found");            return Results.Json(ApiResponse<Metadata>.Ok(result));        }        else        {            logger.LogAlways("HanimeDetail", "❌ Not found");            return Results.Json(ApiResponse<Metadata>.Fail($"Content not found: {id}"));        }    }    catch (OperationCanceledException)    {        logger.LogWarningLite("HanimeDetail", "Cancelled", id);        return Results.Json(ApiResponse<Metadata>.Fail("Request cancelled"));    }    catch (Exception ex)    {        logger.LogFailure("HanimeDetail", $"Detail error: {ex.Message}", id, ex);        return Results.Json(ApiResponse<Metadata>.Fail($"Detail error: {ex.Message}"));    }    finally    {        slot?.Dispose();    }});// =============== DLsite API Endpoints ===============/// <summary>/// Search for DLsite content by title with optional filtering and result limiting./// This endpoint performs content search using the provided title as a search term,/// then fetches detailed metadata for each result concurrently./// </summary>/// <param name="title">Search title or keyword. Will be treated as search text regardless of format.</param>/// <param name="max">Maximum number of results to return (default: 12, max: 50)</param>/// <param name="provider">DLsite provider instance (dependency injected)</param>/// <param name="dlsiteLimiter">DLsite concurrency limiter for resource management</param>/// <param name="dlsiteRateLimiter">DLsite rate limiter for request throttling</param>/// <param name="ct">Cancellation token for request timeout handling</param>/// <returns>JSON response containing list of DLsite metadata objects</returns>app.MapGet("/api/dlsite/search", async (    string title,    int? max,    DlsiteProvider provider,    ScraperBackendService.Core.Concurrency.DlsiteConcurrencyLimiter dlsiteLimiter,    ScraperBackendService.Core.Concurrency.DlsiteRateLimiter dlsiteRateLimiter,    CancellationToken ct) =>{    ConcurrencySlot? slot = null;        try    {        // Log user operation start        logger.LogAlways("DLsiteSearch", $"Searching: '{title}'");                // Wait for available slot (up to 15 seconds)        slot = await dlsiteLimiter.TryWaitAndAcquireSlotAsync(15000, ct).ConfigureAwait(false);        if (slot == null)        {            // Rate limit exceeded - return 429 status            logger.LogRateLimit("DLsiteSearch");            return Results.Json(                ApiResponse<List<Metadata>>.Fail("Service busy. All concurrency slots occupied. Please retry later."),                statusCode: 429);        }                logger.LogDebug("[DLsiteSearch] Acquired slot {SlotId} for search: '{Title}'", slot.SlotId, title);                // Apply rate limiting for this slot before searching        var waitTime = dlsiteRateLimiter.GetWaitTime(slot.SlotId);        if (waitTime > TimeSpan.Zero)        {            // Log rate limit wait time            logger.LogAlways("DLsiteSearch", $"Waiting {waitTime.TotalSeconds:F0}s (rate limit)");        }                await dlsiteRateLimiter.WaitIfNeededAsync(slot.SlotId, ct).ConfigureAwait(false);                // Now perform the search        var maxResults = Math.Min(max ?? 12, 50);        var hits = await provider.SearchAsync(title, maxResults, ct).ConfigureAwait(false);        var results = new List<Metadata>();                // Fetch details for search results (internal concurrent fetching without additional rate limiting)        var tasks = hits.Take(maxResults).Select(async hit =>        {            try            {                var detail = await provider.FetchDetailAsync(hit.DetailUrl, ct).ConfigureAwait(false);                if (detail != null)                {                    lock (results)                    {                        results.Add(detail);                    }                }            }            catch (Exception)            {                logger.LogWarningLite("DetailFetch", "Failed", hit.DetailUrl);            }        });                await Task.WhenAll(tasks).ConfigureAwait(false);                // Log operation result        if (results.Count > 0)        {            logger.LogAlways("DLsiteSearch", $"Found {results.Count} results");        }        else        {            logger.LogAlways("DLsiteSearch", "No results found");        }                return Results.Json(ApiResponse<List<Metadata>>.Ok(results));    }    catch (OperationCanceledException)    {        logger.LogWarningLite("DLsiteSearch", "Cancelled", title);        return Results.Json(ApiResponse<List<Metadata>>.Fail("Request cancelled"));    }    catch (Exception ex)    {        logger.LogFailure("DLsiteSearch", $"Search error: {ex.Message}", title, ex);        return Results.Json(ApiResponse<List<Metadata>>.Fail($"Search error: {ex.Message}"));    }    finally    {        slot?.Dispose();    }});/// <summary>/// Get detailed information for a specific DLsite product by ID./// This endpoint first checks the cache, then fetches from the provider if needed./// Implements two-stage cache checking to minimize concurrency slot usage./// </summary>/// <param name="id">DLsite product ID to retrieve (e.g., "RJ123456", "VJ123456")</param>/// <param name="provider">DLsite provider instance (dependency injected)</param>/// <param name="dlsiteLimiter">DLsite concurrency limiter for resource management</param>/// <param name="dlsiteRateLimiter">DLsite rate limiter for request throttling</param>/// <param name="cache">Metadata cache instance for result caching</param>/// <param name="ct">Cancellation token for request timeout handling</param>/// <returns>JSON response containing DLsite metadata object or error</returns>app.MapGet("/api/dlsite/{id}", async (    string id,    DlsiteProvider provider,    ScraperBackendService.Core.Concurrency.DlsiteConcurrencyLimiter dlsiteLimiter,    ScraperBackendService.Core.Concurrency.DlsiteRateLimiter dlsiteRateLimiter,    MetadataCache cache,    CancellationToken ct) =>{    ConcurrencySlot? slot = null;        try    {        // Log user operation start        logger.LogAlways("DLsiteDetail", $"Query: '{id}'");                if (!provider.TryParseId(id, out var parsedId))        {            logger.LogWarningLite("DLsiteDetail", "Invalid ID format", id);            return Results.Json(ApiResponse<Metadata>.Fail($"Invalid DLsite ID: {id}"));        }                // First, check cache without acquiring concurrency slot        var cachedResult = cache.TryGetCached("dlsite", parsedId);        if (cachedResult != null)        {            // Cache hit - no need for concurrency slot            logger.LogInformation("[DLsiteDetail] Cache hit: '{Id}'", id);                        // Log operation result            logger.LogAlways("DLsiteDetail", $"✅ Found (cache)");            return Results.Json(ApiResponse<Metadata>.Ok(cachedResult));        }                // Cache miss - now we need to acquire concurrency slot        slot = await dlsiteLimiter.TryWaitAndAcquireSlotAsync(15000, ct).ConfigureAwait(false);        if (slot == null)        {            // Rate limit exceeded - return 429 status            logger.LogRateLimit("DLsiteDetail");            return Results.Json(                ApiResponse<Metadata>.Fail("Service busy. All concurrency slots occupied. Please retry later."),                 statusCode: 429);        }                logger.LogDebug("[DLsiteDetail] Acquired slot {SlotId} for '{Id}'", slot.SlotId, id);                // Double-check cache after acquiring slot (in case another request cached it)        cachedResult = cache.TryGetCached("dlsite", parsedId);        if (cachedResult != null)        {            logger.LogInformation("[DLsiteDetail] Cache hit after slot: '{Id}'", id);            logger.LogAlways("DLsiteDetail", $"✅ Found (cache)");            return Results.Json(ApiResponse<Metadata>.Ok(cachedResult));        }                // Cache still empty, apply rate limiting for this slot before fetching        var waitTime = dlsiteRateLimiter.GetWaitTime(slot.SlotId);        if (waitTime > TimeSpan.Zero)        {            // Log rate limit wait time            logger.LogAlways("DLsiteDetail", $"Waiting {waitTime.TotalSeconds:F0}s (rate limit)");        }                await dlsiteRateLimiter.WaitIfNeededAsync(slot.SlotId, ct).ConfigureAwait(false);                // Now fetch from provider        var detailUrl = provider.BuildDetailUrlById(parsedId);        var result = await provider.FetchDetailAsync(detailUrl, ct);                // Cache the result (even if null)        cache.SetCached("dlsite", parsedId, result);                // Force garbage collection after ID query to release resources        var memoryBefore = GC.GetTotalMemory(false);        GC.Collect();        GC.WaitForPendingFinalizers();        GC.Collect();        var memoryAfter = GC.GetTotalMemory(true);        var freedMemory = memoryBefore - memoryAfter;                // Log memory cleanup at debug level        logger.LogResourceEvent("MemoryCleanup", $"Freed {freedMemory / 1024 / 1024}MB", "DLsiteDetail");                // Log operation result        if (result != null)        {            logger.LogAlways("DLsiteDetail", "✅ Found");            return Results.Json(ApiResponse<Metadata>.Ok(result));        }        else        {            logger.LogAlways("DLsiteDetail", "❌ Not found");            return Results.Json(ApiResponse<Metadata>.Fail($"Content not found: {id}"));        }    }    catch (OperationCanceledException)    {        logger.LogWarningLite("DLsiteDetail", "Cancelled", id);        return Results.Json(ApiResponse<Metadata>.Fail("Request cancelled"));    }    catch (Exception ex)    {        logger.LogFailure("DLsiteDetail", $"Detail error: {ex.Message}", id, ex);        return Results.Json(ApiResponse<Metadata>.Fail($"Detail error: {ex.Message}"));    }    finally    {        slot?.Dispose();    }});// Build and start the servicevar listenUrl = $"http://{serviceConfig.Host}:{serviceConfig.Port}";// Always log startup info regardless of logging levellogger.LogAlways("ServiceStartup", $"Listening on {listenUrl}");logger.LogAlways("ServiceStartup", string.IsNullOrWhiteSpace(serviceConfig.AuthToken) ? "Authentication: Disabled" : "Authentication: Enabled", listenUrl);// Register shutdown handlers to always log service stop eventsvar lifetime = app.Lifetime;lifetime.ApplicationStopping.Register(() => logger.LogAlways("ServiceShutdown", "Stopping service"));lifetime.ApplicationStopped.Register(() => logger.LogAlways("ServiceShutdown", "Service stopped"));Console.CancelKeyPress += (_, e) =>{    logger.LogAlways("ServiceShutdown", "CancelKeyPress received - stopping");    // allow default behavior};AppDomain.CurrentDomain.ProcessExit += (_, _) => logger.LogAlways("ServiceShutdown", "Process exiting");try{    app.Run(listenUrl);}catch (Exception ex){    logger.LogFailure("ServiceStartup", "Service failed to start", listenUrl, ex);    throw;}