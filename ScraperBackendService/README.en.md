# ScraperBackendService> **[ä¸­æ–‡](README.md) | English**Core backend service for HanimetaScraper, providing REST API for metadata scraping with **advanced rate limiting and anti-detection capabilities**.## Features### Core Capabilities- **Multi-Provider Support** - Hanime, DLsite with unified interface- **RESTful API** - Standardized JSON responses- **Anti-Bot Protection** - Playwright browser automation with stealth mode- **Smart Caching** - Memory cache with LRU eviction to reduce duplicate requests- **Authentication** - Optional API Token auth for secure deployment### Advanced Rate Limiting- **Concurrency Control** - Configurable provider access limits (default: 3 slots)- **Per-Slot Rate Limiting** - Enforces minimum delay between consecutive requests (default: 30s)- **Request Queuing** - Waits up to 15s for available slots instead of immediate rejection- **Intelligent Throttling** - Prevents IP blocking while maximizing throughput### Performance Optimization- **Aggressive Memory Management** - Optional GC optimization for low-memory environments- **Extended Timeouts** - 150s backend timeout + 180s frontend timeout for complete request flow- **Cache Statistics** - Monitor cache hit rate and performance metrics- **Structured Logging** - Multiple verbosity levels for debugging and monitoring## API Endpoints### Base Endpoints- `GET /` - Service information and authentication status- `GET /health` - Health check endpoint- `GET /cache/stats` - Cache statistics (hit rate, eviction count, etc.)- `DELETE /cache/clear` - Clear all cached entries- `DELETE /cache/{provider}/{id}` - Remove specific cache entry### Hanime Endpoints- `GET /api/hanime/search?title={query}&max={limit}` - Search by title (rate limited)- `GET /api/hanime/{id}` - Get details by ID (rate limited)### DLsite Endpoints- `GET /api/dlsite/search?title={query}&max={limit}` - Search by title (rate limited)- `GET /api/dlsite/{id}` - Get details by ID (rate limited)### Utility Endpoints- `GET /r/dlsite/{id}` - Redirect to DLsite product page## Configuration### Main Configuration Items (appsettings.json)```json{  "ServiceConfig": {    "Port": 8585,    "Host": "0.0.0.0",    "AuthToken": "",    "TokenHeaderName": "X-API-Token",    "HanimeMaxConcurrentRequests": 3,    "DlsiteMaxConcurrentRequests": 3,    "HanimeRateLimitSeconds": 30,    "DlsiteRateLimitSeconds": 30,    "RequestTimeoutSeconds": 150,    "EnableAggressiveMemoryOptimization": true  }}```### Configuration Options Explained| Setting | Description | Default | Recommended Range ||---------|-------------|---------|-------------------|| **Port** | HTTP listening port | 8585 | 1024-65535 || **Host** | Listening address | "0.0.0.0" | "127.0.0.1" (local)/"0.0.0.0" (all interfaces) || **AuthToken** | API authentication token | Empty string | Strong random string (required for production) || **TokenHeaderName** | Auth header name | "X-API-Token" | Custom header name || **HanimeMaxConcurrentRequests** | Hanime concurrent slots | 3 | 1-10 || **DlsiteMaxConcurrentRequests** | DLsite concurrent slots | 3 | 1-10 || **HanimeRateLimitSeconds** | Hanime rate limit per slot | 30 | 10-60 || **DlsiteRateLimitSeconds** | DLsite rate limit per slot | 30 | 10-60 || **RequestTimeoutSeconds** | Request timeout in seconds | 150 | 90-300 || **EnableAggressiveMemoryOptimization** | Enable aggressive GC | true | true/false |### Concurrency Control Explained**Provider Concurrency Limits** - Unified control over provider website access:- `HanimeMaxConcurrentRequests` - Limits simultaneous access to Hanime website- `DlsiteMaxConcurrentRequests` - Limits simultaneous access to DLsite website- Includes all operations: search requests, detail fetching, direct ID queries- Requests wait up to 15s for available slots when limit is reached- Returns 429 status if no slot available after 15s, frontend will auto-retry### Rate Limiting Explained**Per-Slot Independent Rate Limiting** - Provides independent rate control for each concurrency slot:- `HanimeRateLimitSeconds` - Minimum interval between requests for each Hanime slot on cache miss- `DlsiteRateLimitSeconds` - Minimum interval between requests for each DLsite slot on cache miss- Cache hits bypass rate limiting and return immediately- Set to 0 to disable rate limiting- Each slot has its own independent timer, different slots don't interfere**Rate Limiting Workflow:**1. Request arrives â†’ Check cache2. Cache hit â†’ Return immediately (no rate limit) âœ…3. Cache miss â†’ Acquire concurrency slot4. Slot acquired â†’ Check slot's last request time5. If waiting needed â†’ Wait until rate limit interval satisfied6. Fetch from provider â†’ Cache and return### Environment Variable OverridesThe following environment variables can override configuration file settings:| Environment Variable | Corresponding Config | Example ||---------------------|---------------------|---------|| **SCRAPER_PORT** | Port | `8080` || **SCRAPER_AUTH_TOKEN** | AuthToken | `your-secret-token-here` |### Cache ConfigurationCache system is automatically configured with main parameters:- **Cache Capacity**: 100 entries- **Success Result TTL**: 2 minutes- **Failure Result TTL**: 2 minutes- **Eviction Policy**: LRU (Least Recently Used)**Tip:** Improving cache hit rate can significantly boost response speed, as cache hits completely bypass rate limiting.### Performance Tuning Recommendations**Low Load Environment (Personal Use):**```json{  "HanimeMaxConcurrentRequests": 3,  "DlsiteMaxConcurrentRequests": 3,  "HanimeRateLimitSeconds": 20,  "DlsiteRateLimitSeconds": 20}```**High Load Environment (Multi-User):**```json{  "HanimeMaxConcurrentRequests": 10,  "DlsiteMaxConcurrentRequests": 10,  "HanimeRateLimitSeconds": 20,  "DlsiteRateLimitSeconds": 20}```**Conservative Settings (Avoid Blocking):**```json{  "HanimeMaxConcurrentRequests": 1,  "DlsiteMaxConcurrentRequests": 1,  "HanimeRateLimitSeconds": 60,  "DlsiteRateLimitSeconds": 60}```**Disable Rate Limiting (Rely on Concurrency Control Only):**```json{  "HanimeMaxConcurrentRequests": 3,  "DlsiteMaxConcurrentRequests": 3,  "HanimeRateLimitSeconds": 0,  "DlsiteRateLimitSeconds": 0}```### Timeout Configuration Recommendations**RequestTimeoutSeconds Calculation Formula:**```RequestTimeoutSeconds >= Slot Wait Time(15s) + Rate Limit Time(30s) + Scraping Time(60s) + Buffer(45s)Recommended Value: 150 seconds```## Deployment### Development Environment```bashcd ScraperBackendServicedotnet run```### Production Environment```bashcd ScraperBackendServicedotnet publish -c Release -o ./publishcd publishdotnet ScraperBackendService.dll```### Docker```bashdocker build -t hanimeta-scraper-backend .docker run -p 8585:8585 hanimeta-scraper-backend```## Monitoring and Diagnostics### Health Check```bashcurl http://localhost:8585/health```### Cache Statistics```bashcurl http://localhost:8585/cache/stats```### Clear Cache```bashcurl -X DELETE http://localhost:8585/cache/clear```## Troubleshooting### Common Issues**503 Service Unavailable**- Check concurrency slot configuration- Increase `MaxConcurrentRequests` value**Request Timeouts**- Increase `RequestTimeoutSeconds` value- Check network connection stability**Frequent 429 Errors**- Decrease `RateLimitSeconds` value- Increase concurrent slot count**High Memory Usage**- Enable `EnableAggressiveMemoryOptimization`- Reduce cache size### Log LevelsAdjust log levels in `appsettings.json`:```json{  "Logging": {    "LogLevel": {      "Default": "Information",      "ScraperBackendService": "Debug"    }  }}```## Tech Stack- **.NET 8** - Modern C# runtime with performance improvements- **ASP.NET Core** - Web API framework with dependency injection- **Playwright** - Headless browser automation for dynamic content- **HtmlAgilityPack** - Fast HTML parsing and DOM navigation- **System.Text.Json** - High-performance JSON serialization## Security Considerations### Production Deployment1. **Set Strong Auth Token**: `AuthToken` must be a strong random string2. **Restrict Network Access**: Use firewall to limit access IPs3. **Use HTTPS**: Configure reverse proxy (like Nginx) for HTTPS4. **Monitor Logs**: Regularly check for unusual access patterns### Example Nginx Configuration```nginxserver {    listen 443 ssl;    server_name your-domain.com;        ssl_certificate /path/to/cert.pem;    ssl_certificate_key /path/to/key.pem;        location / {        proxy_pass http://127.0.0.1:8585;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;        proxy_set_header X-Forwarded-Proto $scheme;    }}```---**HanimetaScraper Backend Service** - Reliable API service for Jellyfin metadata scraping ðŸš€