using System.Text.RegularExpressions;using HtmlAgilityPack;using Microsoft.Extensions.Logging;using Microsoft.Playwright;using ScraperBackendService.Core.Abstractions;using ScraperBackendService.Core.Net;using ScraperBackendService.Core.Logging;using ScraperBackendService.Core.Normalize;using ScraperBackendService.Core.Parsing;using ScraperBackendService.Core.Routing;using ScraperBackendService.Core.Util;using ScraperBackendService.Models;namespace ScraperBackendService.Providers.Hanime;/// <summary>/// Hanime content provider for scraping anime content metadata from hanime1.me./// Supports both Playwright-based dynamic content extraction and HTML-only fallback parsing./// Implements comprehensive search functionality and detailed metadata extraction./// </summary>/// <remarks>/// This provider handles:/// - Dynamic search results using Playwright for JavaScript-heavy pages/// - Fallback HTML parsing when Playwright is unavailable/// - Detailed metadata extraction including titles, descriptions, ratings, tags, and images/// - Proper resource management and error handling/// - Title cleaning and normalization for frontend display/// </remarks>/// <example>/// Usage example:/// var provider = new HanimeProvider(networkClient, logger);/// /// // Search for content/// var searchResults = await provider.SearchAsync("Love", 10, cancellationToken);/// foreach (var hit in searchResults)/// {///     Console.WriteLine($"Found: {hit.Title} - {hit.DetailUrl}");/// }/// /// // Get detailed information/// var details = await provider.FetchDetailAsync("https://hanime1.me/watch?v=12345", cancellationToken);/// if (details != null)/// {///     Console.WriteLine($"Title: {details.Title}");///     Console.WriteLine($"Rating: {details.Rating}/5");///     Console.WriteLine($"Tags: {string.Join(", ", details.Tags)}");/// }/// </example>public sealed class HanimeProvider : IMediaProvider{    private readonly INetworkClient _net;    private readonly ILogger<HanimeProvider> _log;    private const string Host = "https://hanime1.me";    /// <summary>    /// Initializes a new instance of the HanimeProvider.    /// </summary>    /// <param name="net">Network client for HTTP and browser-based operations</param>    /// <param name="log">Logger for tracking operations and errors</param>    public HanimeProvider(INetworkClient net, ILogger<HanimeProvider> log)    {        _net = net;        _log = log;    }    /// <summary>    /// Gets the provider name identifier.    /// </summary>    public string Name => "Hanime";    /// <summary>    /// Attempts to parse a Hanime ID from the given input string.    /// Supports both direct IDs and URLs containing Hanime content identifiers.    /// </summary>    /// <param name="input">Input string that may contain a Hanime ID (URL or direct ID)</param>    /// <param name="id">Extracted Hanime ID if successful</param>    /// <returns>True if ID was successfully parsed, false otherwise</returns>    /// <example>    /// // Parse from URL    /// if (provider.TryParseId("https://hanime1.me/watch?v=12345", out var id))    /// {    ///     Console.WriteLine($"Extracted ID: {id}"); // Output: "12345"    /// }    ///    /// // Parse from direct ID    /// if (provider.TryParseId("86994", out var id2))    /// {    ///     Console.WriteLine($"Direct ID: {id2}"); // Output: "86994"    /// }    ///     /// // Invalid input    /// if (!provider.TryParseId("invalid-input", out var id3))    /// {    ///     Console.WriteLine("Could not parse ID");    /// }    /// </example>    public bool TryParseId(string input, out string id) => IdParsers.TryParseHanimeId(input, out id);    /// <summary>    /// Builds a detail page URL from a Hanime ID.    /// Constructs the canonical watch URL for the specified content.    /// </summary>    /// <param name="id">Hanime content ID</param>    /// <returns>Complete URL to the detail page</returns>    /// <example>    /// var detailUrl = provider.BuildDetailUrlById("12345");    /// // Returns: "https://hanime1.me/watch?v=12345"    ///     /// var detailUrl2 = provider.BuildDetailUrlById("86994");    /// // Returns: "https://hanime1.me/watch?v=86994"    /// </example>    public string BuildDetailUrlById(string id) => IdParsers.BuildHanimeDetailUrl(id);    /// <summary>    /// Searches for Hanime content using the provided keyword with intelligent result extraction.    /// Uses Playwright for dynamic content when available, with HTML fallback for reliability.    /// Implements deduplication and comprehensive metadata extraction for search results.    /// </summary>    /// <param name="keyword">Search keyword or phrase (supports various languages)</param>    /// <param name="maxResults">Maximum number of results to return</param>    /// <param name="ct">Cancellation token for operation timeout</param>    /// <returns>Read-only list of search hits containing URLs, titles, and cover images</returns>    /// <remarks>    /// This method:    /// 1. Constructs search URL with latest releases sorting    /// 2. Attempts Playwright-based extraction for dynamic content    /// 3. Falls back to HTML parsing if Playwright fails or is unavailable    /// 4. Implements deduplication to avoid duplicate results    /// 5. Extracts titles and cover images when available    /// </remarks>    /// <example>    /// // Search for anime containing "Love"    /// var results = await provider.SearchAsync("Love", 5, CancellationToken.None);    /// foreach (var hit in results)    /// {    ///     Console.WriteLine($"Title: {hit.Title}");    ///     Console.WriteLine($"URL: {hit.DetailUrl}");    ///     if (!string.IsNullOrEmpty(hit.CoverUrl))    ///         Console.WriteLine($"Cover: {hit.CoverUrl}");    /// }    ///     /// // Search with timeout    /// using var cts = new CancellationTokenSource(TimeSpan.FromSeconds(30));    /// var results2 = await provider.SearchAsync("Romance", 10, cts.Token);    /// </example>    public async Task<IReadOnlyList<ScraperBackendService.Core.Abstractions.SearchHit>> SearchAsync(string keyword, int maxResults, CancellationToken ct)    {        var sort = "最新上市"; // Latest releases sorting        var queryParam = $"?query={Uri.EscapeDataString(keyword)}";        var sortParam = $"&sort={Uri.EscapeDataString(sort)}";        var searchUrl = $"{Host}/search{queryParam}{sortParam}";        IPage? page = null;        try        {            page = await _net.OpenPageAsync(searchUrl, ct);            if (page is null)            {                // Fallback to HTML parsing if Playwright is not available                var html = await _net.GetHtmlAsync(searchUrl, ct);                return ParseSearchFromHtml(html, searchUrl, maxResults);            }            // Use Playwright for dynamic content extraction            var itemLocator = page.Locator("div[title] >> a.overlay");                        // Check if any items exist before waiting, with a short timeout            try            {                await itemLocator.First.WaitForAsync(new LocatorWaitForOptions { Timeout = 5000 });            }            catch (TimeoutException)            {                // No search results found, return empty list                _log.LogDebug("HanimeSearch", "No search results found", keyword);                return new List<ScraperBackendService.Core.Abstractions.SearchHit>();            }            var hits = new List<ScraperBackendService.Core.Abstractions.SearchHit>();            var baseUri = new Uri(Host);            var seen = new HashSet<string>(StringComparer.OrdinalIgnoreCase);            int count = await itemLocator.CountAsync();            for (int i = 0; i < count && hits.Count < maxResults; i++)            {                var a = itemLocator.Nth(i);                string? href = await a.GetAttributeAsync("href");                if (string.IsNullOrWhiteSpace(href)) continue;                var detailUrl = new Uri(baseUri, href).AbsoluteUri;                if (!detailUrl.StartsWith($"{Host}/watch", StringComparison.OrdinalIgnoreCase)) continue;                if (!seen.Add(detailUrl)) continue; // Skip duplicates                var container = a.Locator("xpath=ancestor::div[@title][1]");                string title = (await container.GetAttributeAsync("title"))?.Trim() ?? "";                string cover = "";                try                {                    var img = container.Locator("img[src*='/thumbnail/']");                    if (await img.CountAsync() > 0)                        cover = await img.First.GetAttributeAsync("src") ?? "";                }                catch { /* Ignore image extraction errors */ }                hits.Add(new ScraperBackendService.Core.Abstractions.SearchHit(detailUrl, TextNormalizer.Clean(title), cover));            }            // Log search results            if (hits.Count > 0)            {                _log.LogSuccess("HanimeSearch", keyword, hits.Count);            }            return hits;        }        catch (TimeoutException ex)        {            // Handle timeout gracefully by falling back to HTML parsing            _log.LogWarning("HanimeSearch", "Playwright timeout, falling back to HTML parsing", keyword, ex);            try            {                var html = await _net.GetHtmlAsync(searchUrl, ct);                return ParseSearchFromHtml(html, searchUrl, maxResults);            }            catch (Exception fallbackEx)            {                _log.LogFailure("HanimeSearch", "Both Playwright and HTML parsing failed", keyword, fallbackEx);                return new List<ScraperBackendService.Core.Abstractions.SearchHit>();            }        }        finally        {            // Use the network client's ClosePageAsync to properly manage page lifecycle            if (page != null && _net is PlaywrightNetworkClient playwrightClient)            {                await playwrightClient.ClosePageAsync(page);            }        }    }    /// <summary>    /// Parses search results from HTML content when Playwright is not available.    /// Provides reliable fallback mechanism for search functionality using HTML parsing.    /// </summary>    /// <param name="html">HTML content of the search page</param>    /// <param name="baseUrl">Base URL for resolving relative links</param>    /// <param name="maxResults">Maximum number of results to extract</param>    /// <returns>List of search hits parsed from HTML</returns>    /// <remarks>    /// This fallback method:    /// - Parses HTML using HtmlAgilityPack for reliability    /// - Extracts search result links and metadata    /// - Handles various HTML structures and edge cases    /// - Provides consistent result format with Playwright approach    /// </remarks>    /// <example>    /// var html = await httpClient.GetStringAsync("https://hanime1.me/search?query=Love");    /// var results = ParseSearchFromHtml(html, "https://hanime1.me/search", 10);    /// foreach (var hit in results)    /// {    ///     Console.WriteLine($"Found: {hit.Title} - {hit.DetailUrl}");    /// }    /// </example>    private static IReadOnlyList<SearchHit> ParseSearchFromHtml(string html, string baseUrl, int maxResults)    {        // HtmlDocument doesn't implement IDisposable, so we can't use 'using'        var doc = new HtmlDocument();        doc.LoadHtml(html);        var hits = new List<SearchHit>();        var nodes = ScrapingUtils.SelectNodes(doc, "//a[contains(@class,'overlay') and starts-with(@href,'/watch')]");                if (nodes == null || !nodes.Any())        {            // No search results found in HTML            return hits;        }        foreach (var a in nodes)        {            try            {                var href = a.GetAttributeValue("href", "");                if (string.IsNullOrWhiteSpace(href)) continue;                var detailUrl = new Uri(new Uri(Host), href).AbsoluteUri;                var container = a.SelectSingleNode("ancestor::div[@title][1]");                var title = container?.GetAttributeValue("title", "") ?? a.GetAttributeValue("title", "") ?? "";                var cover = container?.SelectSingleNode(".//img[@src]")?.GetAttributeValue("src", "") ?? "";                hits.Add(new SearchHit(detailUrl, TextNormalizer.Clean(title), cover));                if (maxResults > 0 && hits.Count >= maxResults) break;            }            catch (Exception)            {                // Skip invalid entries and continue processing                continue;            }        }        return hits;    }    /// <summary>    /// Fetches detailed metadata for a specific Hanime content with comprehensive information extraction.    /// Uses both Playwright and HTML parsing to extract all available metadata including titles,    /// descriptions, ratings, tags, studio information, and images.    /// </summary>    /// <param name="detailUrl">URL of the detail page to scrape</param>    /// <param name="ct">Cancellation token for operation timeout</param>    /// <returns>Complete metadata object or null if extraction fails or content not found</returns>    /// <remarks>    /// This method performs comprehensive metadata extraction:    /// 1. Validates content existence using key page elements    /// 2. Extracts title information with bracket removal and cleaning    /// 3. Parses description from specific content areas    /// 4. Extracts and normalizes tags/genres    /// 5. Converts rating from percentage to 0-5 scale    /// 6. Extracts studio and release date information    /// 7. Processes video poster images    /// 8. Applies title cleaning for frontend display    /// </remarks>    /// <example>    /// var metadata = await provider.FetchDetailAsync("https://hanime1.me/watch?v=12345", CancellationToken.None);    /// if (metadata != null)    /// {    ///     Console.WriteLine($"Title: {metadata.Title}");    ///     Console.WriteLine($"Original Title: {metadata.OriginalTitle}");    ///     Console.WriteLine($"Description: {metadata.Description}");    ///     Console.WriteLine($"Rating: {metadata.Rating}/5");    ///     Console.WriteLine($"Release Date: {metadata.ReleaseDate}");    ///     Console.WriteLine($"Studio: {string.Join(", ", metadata.Studios)}");    ///     Console.WriteLine($"Tags: {string.Join(", ", metadata.Tags)}");    ///     Console.WriteLine($"Cover: {metadata.Primary}");    ///     Console.WriteLine($"Thumbnails: {metadata.Thumbnails.Count} images");    /// }    /// else    /// {    ///     Console.WriteLine("Content not found or extraction failed");    /// }    /// </example>    public async Task<Metadata?> FetchDetailAsync(string detailUrl, CancellationToken ct)    {        IPage? page = null;        try        {            page = await _net.OpenPageAsync(detailUrl, ct);            string html;                        // Create seed metadata with sourceUrls properly initialized            var seedMeta = new Metadata();            seedMeta.SourceUrls.Add(detailUrl);            if (page is null)            {                html = await _net.GetHtmlAsync(detailUrl, ct);            }            else            {                // Extract title using Playwright locators for better accuracy                await TryFillTitleViaLocatorAsync(page, seedMeta, ct);                html = await page.ContentAsync();            }            var meta = ParseDetailHtml(html, detailUrl, seedMeta);            return meta;        }        finally        {            // Use the network client's ClosePageAsync to properly manage page lifecycle            if (page != null && _net is PlaywrightNetworkClient playwrightClient)            {                await playwrightClient.ClosePageAsync(page);            }        }    }    /// <summary>    /// Attempts to extract the title using Playwright locators for enhanced accuracy.    /// Uses dynamic content extraction when Playwright is available for more reliable title parsing.    /// </summary>    /// <param name="page">Playwright page object</param>    /// <param name="meta">Metadata object to populate with title information</param>    /// <param name="ct">Cancellation token for operation timeout</param>    /// <returns>Task representing the asynchronous operation</returns>    /// <remarks>    /// This method:    /// - Targets specific title elements using Playwright selectors    /// - Removes bracketed annotations from titles    /// - Populates both Title and OriginalTitle fields    /// - Handles extraction errors gracefully    /// </remarks>    /// <example>    /// await TryFillTitleViaLocatorAsync(page, metadata, CancellationToken.None);    /// // metadata.Title and metadata.OriginalTitle will be populated if successful    /// if (!string.IsNullOrEmpty(metadata.Title))    /// {    ///     Console.WriteLine($"Extracted title: {metadata.Title}");    /// }    /// </example>    private static async Task TryFillTitleViaLocatorAsync(IPage page, Metadata meta, CancellationToken ct)    {        try        {            var titleNode = page.Locator("#shareBtn-title");            if (await titleNode.CountAsync() > 0)            {                var raw = (await titleNode.InnerTextAsync()).Trim();                var re = new Regex(@"\[.*?\]");                var clean = re.Replace(raw, string.Empty, 1 /*count*/).Trim();                meta.OriginalTitle = clean;                if (string.IsNullOrWhiteSpace(meta.Title)) meta.Title = clean;            }        }        catch { /* Ignore title extraction errors */ }    }    /// <summary>    /// Parses detailed metadata from HTML content of a Hanime detail page.    /// Performs comprehensive extraction of all available metadata fields including validation,    /// content parsing, and normalization for consistent output format.    /// </summary>    /// <param name="html">HTML content of the detail page</param>    /// <param name="detailUrl">URL of the detail page for reference</param>    /// <param name="seed">Optional seed metadata to merge with parsed data</param>    /// <returns>Complete metadata object with all extracted information, or null if no valid content found</returns>    /// <remarks>    /// This method performs:    /// 1. Content validation using key page elements    /// 2. ID extraction from URL    /// 3. Title extraction and cleaning (removes brackets)    /// 4. Description extraction from specific content areas    /// 5. Tag/genre extraction and normalization    /// 6. Rating conversion from percentage to 0-5 scale    /// 7. Studio information extraction    /// 8. Release date parsing    /// 9. Image URL extraction (video poster)    /// 10. Title cleaning for frontend display    /// </remarks>    /// <example>    /// var html = await httpClient.GetStringAsync("https://hanime1.me/watch?v=12345");    /// var metadata = ParseDetailHtml(html, "https://hanime1.me/watch?v=12345", null);    /// if (metadata != null)    /// {    ///     Console.WriteLine($"Extracted {metadata.Tags.Count} tags");    ///     Console.WriteLine($"Rating: {metadata.Rating}/5");    ///     Console.WriteLine($"Studio: {string.Join(", ", metadata.Studios)}");    ///     Console.WriteLine($"Has description: {!string.IsNullOrEmpty(metadata.Description)}");    /// }    /// else    /// {    ///     Console.WriteLine("Invalid or missing content");    /// }    /// </example>    private Metadata? ParseDetailHtml(string html, string detailUrl, Metadata? seed)    {        // HtmlDocument doesn't implement IDisposable, so we can't use 'using'        var doc = new HtmlDocument();        doc.LoadHtml(html);        // First, check if this is a valid Hanime content page        // Look for key elements that indicate a valid content page        var titleNode = doc.DocumentNode.SelectSingleNode("//h3[@id='shareBtn-title']");        var videoPlayer = doc.DocumentNode.SelectSingleNode("//video[@poster]");        var tagDivs = doc.DocumentNode.SelectNodes("//div[@class='single-video-tag']");        // If we don't find any key content indicators, this is likely a 404 or invalid page        if (titleNode == null && videoPlayer == null && (tagDivs == null || tagDivs.Count == 0))        {            _log.LogWarning("ParseDetailHtml", "No valid Hanime content found", detailUrl);            return null;        }        var meta = seed ?? new Metadata();                // Ensure SourceUrls is initialized and add detailUrl only if not already present        if (meta.SourceUrls == null)        {            meta.SourceUrls = new List<string>();        }        if (!meta.SourceUrls.Contains(detailUrl))        {            meta.SourceUrls.Add(detailUrl);        }                if (IdParsers.TryExtractHanimeIdFromUrl(detailUrl, out var id)) meta.ID = id;        // Extract title - remove brackets and clean text        if (titleNode != null)        {            var raw = titleNode.InnerText.Trim();            var re = new Regex(@"\[.*?\]");            var clean = re.Replace(raw, string.Empty, 1 /*count*/).Trim();            meta.OriginalTitle = clean;            if (string.IsNullOrWhiteSpace(meta.Title)) meta.Title = clean;        }        // Extract description - target specific description area        var capDiv = doc.DocumentNode.SelectSingleNode("//div[@class='video-caption-text caption-ellipsis']");        if (capDiv != null)        {            meta.Description = capDiv.InnerText?.Trim();        }        else        {            // Fallback approach for description            capDiv = doc.DocumentNode.SelectSingleNode("//div[contains(@class, 'video-caption-text')]");            if (capDiv != null)            {                meta.Description = capDiv.InnerText?.Trim();            }        }        // Extract tags/genres - exclude interactive elements        var tagDivs2 = doc.DocumentNode.SelectNodes("//div[@class='single-video-tag' and not(@data-toggle) and not(@data-target)]");        if (tagDivs2 != null)        {            var tagSet = new HashSet<string>(StringComparer.OrdinalIgnoreCase);            foreach (var div in tagDivs2)            {                var a = div.SelectSingleNode("./a");                string tag;                if (a != null)                {                    var textNode = a.SelectSingleNode("text()");                    tag = (textNode?.InnerText ?? a.InnerText) ?? "";                }                else tag = div.InnerText ?? "";                tag = CleanTag(tag);                if (!string.IsNullOrWhiteSpace(tag)) tagSet.Add(tag);            }            meta.Tags = tagSet.ToList();        }        // Extract rating: convert percentage to 0-5 scale        var likeDiv = doc.DocumentNode.SelectSingleNode("//div[@id='video-like-form-wrapper']//div[contains(@class,'single-icon')]");        if (likeDiv != null)        {            var text = string.Concat(likeDiv.ChildNodes.Where(n => n.NodeType == HtmlNodeType.Text).Select(n => n.InnerText)).Trim();            var m = Regex.Match(text, @"(\d+)%");            if (m.Success && int.TryParse(m.Groups[1].Value, out var percent))                meta.Rating = percent / 20.0; // Convert percentage to 0-5 scale        }        // Extract studio information        var artistNode = doc.DocumentNode.SelectSingleNode("//a[@id='video-artist-name']");        if (artistNode != null)        {            var studio = artistNode.InnerText.Trim();            if (!string.IsNullOrWhiteSpace(studio) && !meta.Studios.Contains(studio))                meta.Studios.Add(studio);        }        // Extract release date        var descDiv = doc.DocumentNode.SelectSingleNode("//div[contains(@class, 'video-description-panel-hover')]");        if (descDiv != null)        {            var m = Regex.Match(descDiv.InnerText, @"\d{4}-\d{2}-\d{2}");            if (m.Success && DateTimeOffset.TryParse(m.Value, out var dt))            { meta.ReleaseDate = dt; meta.Year = dt.Year; }        }        // Extract images - focus on video poster only        if (videoPlayer != null)        {            var poster = videoPlayer.GetAttributeValue("poster", "");            if (!string.IsNullOrWhiteSpace(poster))            {                // Use the same image as both Primary and Thumbnail                meta.Primary = poster;                meta.Backdrop = poster;                meta.Thumbnails.Add(poster);            }        }        // Clean titles before returning to frontend        if (!string.IsNullOrWhiteSpace(meta.Title))        {            meta.Title = TitleCleaner.CleanTitle(meta.Title);        }        if (!string.IsNullOrWhiteSpace(meta.OriginalTitle))        {            meta.OriginalTitle = TitleCleaner.CleanTitle(meta.OriginalTitle);        }        return meta;    }    /// <summary>    /// Cleans tag text by removing special characters and normalizing whitespace.    /// Handles various forms of quotes, colons, and HTML entities commonly found in tag text.    /// </summary>    /// <param name="raw">Raw tag text to clean</param>    /// <returns>Cleaned and normalized tag text</returns>    /// <remarks>    /// This method removes:    /// - Various quote characters (ASCII and Unicode)    /// - Colons and special punctuation    /// - HTML entities like &amp;nbsp;    /// - Extra whitespace    /// </remarks>    /// <example>    /// var cleaned = CleanTag("\"Romance\": "); // Returns: "Romance"    /// var cleaned2 = CleanTag("Comedy&nbsp;"); // Returns: "Comedy"    /// var cleaned3 = CleanTag(""Drama"："); // Returns: "Drama"    /// var cleaned4 = CleanTag("  Action  "); // Returns: "Action"    /// </example>    private static string CleanTag(string raw)    {        if (string.IsNullOrWhiteSpace(raw)) return "";        return raw.Replace("\"", "")                  .Replace('\u201C'.ToString(), "") // Left double quote                  .Replace('\u201D'.ToString(), "") // Right double quote                  .Replace("：", "")                  .Replace(":", "")                  .Replace("\u00A0", "")                  .Replace("&nbsp;", "")                  .Trim();    }}